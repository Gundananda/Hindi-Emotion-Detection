# ğŸª” Hindi Emotion Recognition with mBERT Embeddings + BiLSTM (TensorFlow/Keras)

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python&logoColor=white)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?logo=tensorflow&logoColor=white)](https://www.tensorflow.org/)
[![Transformers](https://img.shields.io/badge/Transformers-ğŸ¤—-yellow)](https://huggingface.co/transformers/)
[![PyTorch (embeddings)](https://img.shields.io/badge/PyTorch-embed_only-EE4C2C?logo=pytorch&logoColor=white)](https://pytorch.org/)
[![imblearn](https://img.shields.io/badge/imbalanced--learn-SMOTE-7B1FA2)](https://imbalanced-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

Endâ€‘toâ€‘end Hindi emotion classification using mBERT meanâ€‘pooled sentence embeddings, class rebalancing with SMOTE, and a lightweight BiLSTM classifier.

</div>

---

## ğŸ“Œ Overview

This repository builds a 5â€‘class Hindi emotion recognizer from an Excel dataset:
- Cleans/tokenizes Hindi text (stopwords + simple stemming)
- Extracts multilingual BERT embeddings (bert-base-multilingual-cased)
- Balances classes in embedding space with SMOTE
- Trains a BiLSTM classifier on the embeddings
- Evaluates with classification report, confusion matrix, and ROC curves

Note: Research/education project.

---

## âœ¨ Key Features

| Feature | Description |
| :--- | :--- |
| ğŸ”¡ Hindi NLP Prep | Indic tokenization, Hindi stopwords removal, and simple suffix-based stemming. |
| ğŸŒ mBERT Embeddings | Meanâ€‘pooled token embeddings from bert-base-multilingual-cased (768â€‘D). |
| âš–ï¸ Class Balancing | SMOTE oversampling in embedding space for balanced training data. |
| ğŸ§  Classifier | BiLSTM(64) â†’ BN â†’ Dropout â†’ Dense(32) â†’ BN â†’ Dropout â†’ Softmax(5). |
| ğŸ“Š Evaluation | Accuracy, precision/recall/F1 (per class + macro/weighted), confusion matrix, multiâ€‘class ROC. |

---

## ğŸ“‚ Project Structure

```plaintext
hindi-emotion-bilstm/
â”œâ”€â”€ HindiBilstmFinal.ipynb        # Main notebook (embeddings â†’ SMOTE â†’ train â†’ eval)
â”œâ”€â”€ Bhaav-Dataset.xlsx            # Dataset (not tracked in LFS by default)
â”œâ”€â”€ HindiEmotion.h5               # Trained Keras model (optional artifact)
â”œâ”€â”€ app.py                        # Simple inference script/app (optional)
â”œâ”€â”€ REVIEW_3.pdf                  # Report/notes (optional)
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ“¦ Dataset

- File: Bhaav-Dataset.xlsx
- Columns:
  - Sentence: Hindi text
  - Annotation: integer label in [0..4]
- Example mapping used in notebook (update to match your sheet):
  - 0 â†’ anger
  - 1 â†’ joy
  - 2 â†’ sad
  - 3 â†’ surprise
  - 4 â†’ neutral

Tip: Ensure the mapping is consistent everywhere (plots, label names, confusion matrix).

---

## ğŸ§  Technical Details

- Embeddings
  - Model: bert-base-multilingual-cased (Hugging Face)
  - Tokenization: max_length=50, truncation/padding to fixed length
  - Pooling: mean across token embeddings â†’ 768â€‘D vector per sentence
- Rebalancing
  - SMOTE(random_state=42) on the 768â€‘D embeddings
- Split
  - Stratified 80/20 train/test after SMOTE
- Classifier
  - Input: (1, 768) sequence per sample
  - BiLSTM(64, return_sequences=False) â†’ BatchNorm â†’ Dropout(0.5)
  - Dense(32, relu) â†’ BatchNorm â†’ Dropout(0.5)
  - Dense(5, softmax)
  - Loss: sparse_categorical_crossentropy
  - Optimizer: Adam
  - Epochs: 100, batch size: 64
- Reported (your run)
  - Val accuracy â‰ˆ 0.906 (see notebook for perâ€‘class metrics)

Note: SMOTE on embeddings is pragmatic but can introduce artifacts; validate on a naturally distributed holdâ€‘out if available.

---

## ğŸš€ Getting Started

### Installation
```bash
pip install tensorflow tensorflow-hub tensorflow-text -U
pip install transformers sentencepiece
pip install torch               # required because embeddings use AutoModel (PyTorch)
pip install imbalanced-learn
pip install indic-nlp-library
pip install openpyxl matplotlib seaborn scikit-learn tqdm
```

### Run the Notebook
1) Place Bhaav-Dataset.xlsx at the configured path (e.g., /content/Bhaav-Dataset.xlsx).
2) Execute cells in HindiBilstmFinal.ipynb:
   - Load + inspect dataset
   - Download Hindi stopwords (or provide stopwords-hi.txt locally)
   - Tokenize/clean (optional for analysis)
   - Encode with mBERT â†’ X_bert (N, 768)
   - SMOTE â†’ train/test split
   - Train BiLSTM and evaluate

---

## ğŸ§ª Inference

Example snippet to predict a single sentence with the trained model and the same mBERT encoder:

```python
import numpy as np, torch
from transformers import AutoTokenizer, AutoModel
from tensorflow.keras.models import load_model

labels = {0:'anger',1:'joy',2:'sad',3:'surprise',4:'neutral'}
tok = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
bert = AutoModel.from_pretrained("bert-base-multilingual-cased").eval()
clf = load_model("HindiEmotion.h5")

def encode(text):
    inp = tok([text], return_tensors="pt", max_length=50, truncation=True, padding="max_length")
    with torch.no_grad():
        out = bert(**inp).last_hidden_state  # (1, seq, 768)
        vec = out.mean(dim=1).cpu().numpy()  # (1, 768)
    return vec.reshape(1, 1, 768)            # match BiLSTM input

x = encode("à¤†à¤œ à¤®à¤¨ à¤¬à¤¹à¥à¤¤ à¤–à¥à¤¶ à¤¹à¥ˆ")
probs = clf.predict(x)[0]
pred = labels[int(probs.argmax())]
print(pred, probs.max())
```

Ensure the label order matches the model trained mapping.

---

## âš–ï¸ Limitations

- mBERT sentence meanâ€‘pooling is simple; CLS pooling or sentence transformers may perform better.
- SMOTE on embeddings may not always reflect real text distributions.
- Basic Hindi stemmer and stopword list are heuristic; consider a stronger pipeline (morph analyzers, contextual normalization).
- Evaluate fairness across topics and dialects; avoid deployment without thorough review.

---

## ğŸ§© Next Steps

- Replace mean pooling with CLS token or use sentenceâ€‘transformers (e.g., paraphrase-multilingual-MiniLM-L12-v2).
- Add early stopping and LR schedules; tune epochs/batch size.
- Try class weights or focal loss instead of SMOTE.
- Calibrate probabilities (Platt/temperature scaling) for downstream use.

---

## ğŸ§ª Reproducibility

- Fix random seeds and log package versions.
- Save:
  - stopwords-hi.txt
  - label mapping
  - trained model (HindiEmotion.h5)
- Keep tokenization/max_length identical between train and inference.

---

## ğŸ“„ License

Released under the MIT License. See LICENSE.


â­ï¸ If this repo helps, a star would be appreciated!
